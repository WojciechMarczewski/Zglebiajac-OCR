# ZGŁĘBIAJĄC OCR

## Definicja i historia technologii OCR

Optyczne rozpoznawanie znaków (ang. _OCR – Optical Character Recognition)_ polega na elektrycznym lub mechanicznym tłumaczeniu obrazów tekstu maszynowego, pisma ręcznego lub tekstu drukowanego na tekst zakodowany maszynowo (zrozumiały dla komputerów) \[1\]. Jest to technologia umożliwiająca automatyczne rozpoznawanie i przetwarzanie tekstu z obrazów czy skanów dokumentów, która znalazła szerokie zastosowanie w procesach biznesowych wymagających wprowadzania danych do systemów informatycznych, oszczędzając czas i redukując błędy wynikające z manualnego wprowadzania takich danych \[2\].

Technologia OCR zaczęła się rozwijać od lat 50 XX w., kiedy to David Shepard zbudował urządzenie zdolne do rozpoznawania każdego z 26 drukowanych znaków alfabetu łacińskiego. Urządzenie przyjęło nazwę „Gismo”, a kilka lat później zostało wykorzystane jako podwalina pod Maszynę Farringtona – urządzenie wykorzystywane do automatycznego sczytywania adresów listów w oddziałach placówek pocztowych w Stanach Zjednoczonych \[3\] \[N1\]. Już wcześniej jednak ludzie fantazjowali o urządzeniach zdolnych do ‘rozumienia’ pisma i liczb, a koncepcje typu Optofon z 1919 roku autorstwa Edmunda Edwarda Fournier d’Albe’a, Reading Machine z późnych lat 20 XX w. od Gustawa Taushecka czy Statistical Machine inwencji Paula W. Handela z 1933 roku są najlepszym przykładem pierwszych kroków w tej dziedzinie. Wymienione maszyny (w tym Gismo) wykorzystywały zjawisko fotoelektryczności do wykrywania różnic w odbijanym świetle jasnych i ciemnych powierzchni dzięki zainstalowanej fotokomórce (najczęściej z selenu), której przewodność elektryczna zmieniała się wraz z intensywnością padającego na nią światła \[4\] \[5\] \[6\]. Każda litera alfabetu miała swoje widmo, dzięki któremu dało się ją skategoryzować i rozróżnić od reszty, a następnie wykorzystać przy na przykład generowaniu dźwięku umożliwiając czytanie liter osobom niewidomym (Optofon, Reading Machine), czy zastosować przy sortowaniu dokumentów w biurach (Statistical Machine).

W latach 60 XX w. pojawiły się pierwsze biznesy oparte na technologii OCR z wykorzystaniem komputerów. Początkowo urządzenia były mocno ograniczone w rozpoznawaniu kształtów oraz znaków i do sprawnego działania wymagały opracowania własnej czcionki skierowanej dla maszyn. Rozpoznawanie znaków opierało się na porównywaniu obrazów z biblioteką prototypów dla każdego znaku. Przez ówczesne ograniczenia rozwiązania OCR były zazwyczaj używane w celu odczytywania cyfr i podstawowych symboli. Do gry wkroczyła także firma IBM, opracowując własną czcionkę, lecz w bardziej zaawansowanej formie oraz stosując specjalną matrycę (ang. _move record_), która ograniczała ryzyko błędu wynikające ze zmiany pozycji znaków, co pozwoliło poprawić dokładność i efektywność identyfikacji znaków w porównaniu do poprzedników. Lata 1960-1965 można uznać za pierwszą generację technologii OCR, charakteryzującą się ograniczonymi możliwościami odczytywania znaków oraz specjalnymi, mało naturalnymi, czcionkami do odczytu maszynowego \[3\].

Druga generacja OCR w latach 1965 – 1975 charakteryzowała się usprawnianiem działania technologii, co umożliwiło rozpoznawanie zwykłego pisma maszynowego, a także w ograniczonym stopniu rozpoznawanie znaków ręcznie pisanych. Firma IBM została głównym graczem na rynku technologii OCR, a jej nowatorskie urządzenie IBM 1287 Optical Reader stało się powszechnie rozpoznawalne. We wspomnianym okresie przeprowadzano prace nad normalizacją czcionek, czego efektem było opracowanie nowych i bardziej czytelnych dla ludzi czcionek OCR-A i OCR-B \[3\].

Trzecią generację stanowiły lata 1975-1985. Badania i rozwój technologii skupiały się na zmniejszaniu kosztów i zwiększaniu wydajności. Dokumenty niskiej jakości oraz duże drukowane oraz ręcznie pisane sekwencje znaków nadal stanowiły wyzwanie dla ówczesnych rozwiązań \[3\]. Często wymienianą osobą z tego okresu jest Ray Kurzweil, który w latach 1975-76 połączył trzy nowe technologie – omni-font OCR przystosowany do odczytywania znaków z różnych czcionek, płaskie skanery z matrycą CCD (ang. _Charge-coupled device_) oraz technologię syntezy tekstu w mowę (ang. _TTS – Text-To-Speech_), tworząc produkt zwany Kurzweil Reading Machine \[N2\]. Innowacja ta pozwoliła osobom niewidomym słuchać książek, magazynów czy innych dokumentów. Metody i techniki używane w tym okresie do rozpoznawania znaków zostały usprawnione dzięki matematycznym formułom standaryzującym oraz klasyfikującym opartym na teorii prawdopodobieństwa \[2\].

Uważa się, że czwarta generacja trwa do dziś, a technologia OCR w tej generacji potrafi sobie poradzić z dokumentami zawierającymi tekst z grafiką, tabelami czy matematycznymi symbolami. Pismo odręczne, kolorowe dokumenty czy dokumenty o niskiej jakości także nie stanowią problemu dla tej technologii \[3\] \[7\]. Jednakże, technologia OCR nie wygląda dzisiaj podobnie do tej sprzed roku 2000, kiedy to optyczne rozpoznawanie znaków skupiało się bardziej na segmentacji każdego znaku oraz redukcji wymiarowości w obrazach w celu zmniejszenia trudności identyfikacji znaków w sekwencjach słów. Na początku lat 90. XX wieku opracowano specjalne algorytmy komplementarne, które zwiększały zdolności do odczytywania pisma drukowanego. Niestety, pismo odręczne wciąż stanowiło duże wyzwanie dla tej technologii. W latach 90. XX wieku pojawiły się również pierwsze koncepcje wykorzystania sieci neuronowych w optycznym rozpoznawaniu znaków, co zapoczątkowało rozwój technologii używanej obecnie. Tak więc od lat 90. sieci neuronowe stały się kluczowym obszarem badań, a rozwijane metody uczenia maszynowego systematycznie usprawniały działanie technologii OCR. 

Rozwój możliwości obliczeniowych komputerów pozwolił po 2010 roku na zastosowanie konwolucyjnych sieci neuronowych (ang. _CNN – Convolutional Neural Network_) czy rekurencyjnych sieci neuronowych (ang. _RNN – Recurrent Neural Network_) – technik z dziedziny uczenia głębokiego – w technologii optycznego rozpoznawania znaków, rozpoczynając kolejną falę ulepszeń i rozwoju silników OCR. W ostatnich latach badania nad technologią OCR nastawione są na identyfikację tekstu uszkodzonych dokumentów zawierających tylko fragmenty pisma i znaków \[2\]. 

Mimo powszechnej opinii, że OCR jest już dojrzałą technologią, dzisiejsze silniki wymagają ogromnych kolekcji danych treningowych w postaci par obraz – tekst oraz znacznych zasobów obliczeniowych. Przez wymienione wcześniej ograniczenia, zastosowanie OCR do języków z mniejszą ilością danych jest wyzwaniem, a nawet najlepiej działające silniki OCR potrafią błędnie rozpoznać połowę znaków z japońskich dokumentów z lat 50 XX wieku \[8\]. Wiele nowych artykułów naukowych na temat OCR traktuje także o korekcji błędów po procesach rozpoznawania znaków z wykorzystaniem dużych modeli językowych (ang. _LLM – Large Language Model_) w tak zwanych procesach post-ocr.

Technologię OCR można podzielić ze względu na sposób pozyskiwania tekstu z obrazu na systemy online i offline. Systemy online rozpoznają znaki i słowa w czasie rzeczywistym z urządzeń obsługujących rysiki i ekrany dotykowe, takich jak tablety, smartfony czy pady. Systemy offline natomiast przetwarzają obrazy pochodzące z kamer, skanerów i innych optycznych urządzeń \[9\].

## Przegląd zastosowań OCR w różnych dziedzinach

OCR można zaadoptować wszędzie tam, gdzie pojawia się tekst pisany, co sprawia, że znajduje zastosowanie w wielu różnych dziedzinach, usprawniając i automatyzując procesy przetwarzania tekstu. Poniżej przedstawiona jest lista najważniejszych zastosowań technologii OCR:

- **Digitalizacja drukowanych i ręcznie pisanych materiałów:** OCR umożliwia przekształcanie książek, czasopism i dokumentów w cyfrowe pliki tekstowe.
- **Automatyzacja wprowadzania danych:** Firmy mogą używać OCR do automatycznego wprowadzania danych z papierowych formularzy do systemów komputerowych.
- **Usprawnienie dostępności:** OCR pomaga w tworzeniu dostępnych wersji dokumentów dla osób niewidomych lub niedowidzących, umożliwiając im korzystanie z czytników ekranu.
- **Przeszukiwanie dokumentów:** Dzięki OCR można przeszukiwać zeskanowane dokumenty pod kątem konkretnych słów lub fraz, co znacznie ułatwia zarządzanie dokumentacją.
- **Przetwarzanie faktur i paragonów:** OCR jest używane do automatycznego przetwarzania faktur i paragonów, co przyspiesza księgowość i zarządzanie finansami.
- **Rozpoznawanie tablic rejestracyjnych:** W systemach monitoringu i kontroli ruchu drogowego OCR jest wykorzystywane do rozpoznawania tablic rejestracyjnych pojazdów.
- **Tłumaczenie tekstów**: OCR w połączeniu z oprogramowaniem do tłumaczeń może automatycznie tłumaczyć teksty z różnych języków.
- **Bankowość**: OCR jest szeroko stosowane w bankowości do automatyzacji różnych procesów. Przykładem jest użycie OCR w bankomatach oraz przy obsłudze czeków, gdzie ręcznie wypisane czeki są skanowane i weryfikowane.
- **CAPTCHA**: CAPTCHA (ang. _Completely Automated Public Turing test to tell Computers and Humans Apart_) to programy generujące testy, które są łatwe do rozwiązania dla ludzi, ale trudne dla komputerów. OCR może być używane do rozpoznawania tekstu  
    w obrazach CAPTCHA, co ma dwie strony medalu, bowiem może być wykorzystywane przez hakerów, ale też przydatne w badaniach w walce z atakami hackerskimi, takimi jak ataki słownikowe i automatyczne fałszywe rejestracje.
- **Optyczne rozpoznawanie nut (OMR)**: OMR jest technologią używaną do skanowania i przekształcania zapisu nutowego z formy drukowanej lub ręcznie pisanej na format cyfrowy lub bezpośrednio w muzykę.
- **Robotyzacja Procesów Biznesowych (RPA):** OCR jest używane do „screen scrapingu”, czyli automatycznego zbierania danych z ekranów aplikacji, które nie oferują interfejsów API do bezpośredniego dostępu do danych. Dzięki temu roboty RPA mogą odczytywać tekst z ekranów i przetwarzać go dalej.

## Zasada działania technologii OCR

Technologia OCR na przestrzeni lat znacząco się zmieniała – początkowo polegała na prostym porównywaniu zeskanowanych znaków ze wzorcami zapisanymi w bazie danych. Wprowadzenie komputerów osobistych i wzrost mocy obliczeniowej umożliwiły zastosowanie bardziej zaawansowanych algorytmów klasyfikujących cechy znaków, takie jak kształty i puste przestrzenie, a implementacja sztucznej inteligencji i uczenia maszynowego znacznie skomplikowała zasady działania tej technologii, jednocześnie zwiększając skuteczność i zakres zastosowań.

Dzisiejsze rozwiązania OCR posiadają wiele wbudowanych modułów wspomagających proces rozpoznawania znaków, jak na przykład techniki preprocessingu, przetwarzające graficznie obrazy z tekstem na bardziej użyteczne do przetworzenia przez OCR. Dalsze dobudowania sprawiły, że proces optycznego rozpoznawania znaków ma charakter sekwencyjny, w którym kolejne moduły odpowiadają za specyficzne zadanie w procesie obróbki danych. Uproszczony przykładowy model procesu OCR dla współczesnych rozwiązań przedstawia Rys. 1.

<div align="center">
  <img src="https://github.com/user-attachments/assets/e850faf7-45ca-4f3a-a5e6-4b9de3a06949" alt="Uproszczony_model_procesu_ocr">
</div>
<sub><i>Rysunek 1 Uproszczony model procesu OCR. Źródło: Opracowanie własne, na podstawie: Jang, Seung Ju. "Ocr related technology trends." European Journal of Engineering and Technology Vol 8.1 (2020)</i></sub>


Obróbka wstępna (ang. _Preprocessing_) polega na przygotowaniu surowego obrazu do analizy przez poprawę jego jakości. Najczęściej polega na usunięciu lub zminimalizowaniu różnych rodzajów zniekształceń i szumów z obrazów zawierających tekst, co wspiera ekstrakcję istotnych cech i zmniejsza obciążenie obliczeniowe kolejnych etapów przetwarzania \[10\] \[11\].

Często stosowaną metodą poprawy jakości obrazu jest redukcja kolorów z przestrzeni RGB do skali szarości, a następnie binaryzacja przetworzonego obrazu. Proces ten polega na przypisaniu pikselowi wartości 0 (czarny) lub 1 (biały) na podstawie ustalonego progu jasności, który określa, czy poziom nasycenia piksela przekracza zadaną wartość. Dzięki temu kontrast między jasnymi a ciemnymi obszarami zostaje wzmocniony, co ułatwia przetwarzanie tekstu przez silniki OCR \[1\].



Obrazy poddawane przetwarzaniu OCR często cechują się niską jakością i licznymi zniekształceniami. Redukcja szumów ma na celu zmniejszenie wpływu niedoskonałości, a przez lata opracowano wiele metod radzenia sobie z tym problemem.

Klasycznym podejściem jest zastosowanie filtrowania przestrzennego, które polega na wpływaniu na wartość głównego piksela za pomocą sąsiadujących pikseli poprzez nałożenie maski filtra o określonych rozmiarach (np. 3x3 piksele), a następnie wykonywaniu operacji statystycznych, takich jak uśrednianie, wyznaczanie wartości środkowych (mediana) czy zastosowanie rozkładu Gaussa \[12\] \[13\].

W celu redukcji szumów stosuje się także techniki transformacyjne, które wykorzystują takie metody jak transformacja Fouriera czy transformacja falkowa. Techniki transformacyjne przekształcają obraz do innej domeny (częstotliwościowej) - jest to przestrzeń, w której sygnał obrazu jest reprezentowany w postaci częstotliwości zamiast oryginalnej przestrzeni pikselowej. W przestrzeni częstotliwościowej szum i użyteczne sygnały mają różne charakterystyki i mogą być łatwiej oddzielone od siebie za pomocą odpowiednich filtrów, progów czy innych metod tłumienia. Po redukcji szumów sygnał jest przekształcany z powrotem do domeny przestrzennej \[12\].

Nowoczesne metody redukcji szumów oparte są na głębokim uczeniu, które dzięki treningowi, mogą usuwać zniekształcenia. Stosowane są perceptrony wielowarstwowe, konwolucyjne sieci neuronowe czy generatywne sieci przeciwstawne (_ang._ _GAN – Generative Adversial Network_) \[12\].

W systemach rozpoznawania tekstu korekcja pochylenia jest kluczowym elementem standaryzacji tekstu, co może poprawić efektywność rozpoznawania znaków. Korekcję wykonuje się w fazie preprocessingu. Wykorzystuje się różne metody, takie jak detekcja linii prostych, klastrowanie (grupowanie) oparte na zasadzie k-najbliższych sąsiadów, metody projekcyjne oraz oparte na konturach \[14\].

Metoda projekcyjna polega na wykrywaniu pochylenia obrazu przez analizę poziomych i pionowych projekcji. Uwzględnia statystyki takie jak wariancja, średnie odchylenie kwadratowe, wektor cech projekcji oraz pole kierunków gradientu. Ta metoda jest skuteczna dla prostych obrazów tekstowych, ale jej dokładność spada wraz ze wzrostem złożoności układu obrazu.

Metoda detekcji linii prostych wykrywa linie proste w obrazie za pomocą algorytmów transformacyjnych, oblicza kąt nachylenia, a następnie obraca obraz o ten kąt.

Metoda k-najbliższych sąsiadów znajduje k najbliższych sąsiadów dla wszystkich połączonych centrów, oblicza kierunek wektora każdej pary sąsiadów i generuje histogram, którego szczyt odpowiada nachyleniu całego obrazu.

Metoda oparta na konturach polega na obliczaniu kąta nachylenia na podstawie konturów obrazu, ale ma ograniczenie, gdy brakuje regularnych granic.

Analiza regionów to kolejny etap przygotowania obrazu i ma na celu prawidłowe rozpoznanie tekstu w przetwarzanym obrazie poprzez różne techniki analizy jasności, kolorów, tekstur i kształtów obiektów. Dzięki segmentacji regionów z tekstem, na przykład przez wycinanie, moduł rozpoznawania (faktyczny OCR) może łatwiej radzić sobie z identyfikacją znaków. Gdy obraz zawiera dużo tekstu, OCR może mieć trudności  z rozpoznaniem, ponieważ zazwyczaj jest uczony rozpoznawania znaków z jednej linii tekstu lub pojedynczych znaków. Segmentacja i sekwencyjne przetwarzanie regionów tekstu pozwala na skuteczniejsze przetwarzanie dużej ilości informacji \[15\]. Zazwyczaj wyróżnia się dwie fazy tego procesu: separację linii tekstu oraz separację słów i znaków.

Segmentacja linii tekstu ma na celu wyizolowanie ciągów znaków z obrazu poprzez różne techniki TLD (ang. Text Line Detection), takie jak algorytmy wykrywania krawędzi obiektów, analizy histogramów \[11\], projekcji profilu oraz metody klastrowania. Wśród stosowanych technik znajdują się także metody oparte na połączonych komponentach (ang. _CC-based_), rozmazywania (ang. _smearing_), obwiednie (ang. _bounding_), transformacja Hougha, techniki kombinowane oraz metody oparte na głębokim uczeniu \[8\].

Segmentacja słów i znaków rozdziela wcześniej wyizolowane ciągi tekstu na wyrazy i pojedyncze litery, co ma istotny wpływ na sukces etapów ekstrakcji cech i rozpoznawania znaków. Wydajność tego procesu jest silnie uzależniona od specyfiki danego języka, ponieważ w niektórych przypadkach znaki mogą się stykać i nakładać na siebie. Istnieją dwa podejścia do segmentacji. Jawna segmentacja znaków, która wyodrębnia każdy znak z linii tekstu, oraz niejawna segmentacja, która polega na jednoczesnym przeprowadzaniu etapów segmentacji i rozpoznawania znaków bez wcześniejszego dzielenia na mniejsze jednostki. Oba podejścia mają swoje zastosowania w zależności od specyfiki systemu OCR  i języka, dla którego są używane. W niektórych przypadkach oba etapy mogą być przeprowadzane jednocześnie, co zwiększa efektywność procesu \[9\].

Jawna segmentacja dzieli słowa na mniejsze jednostki, takie jak ligatury, znaki lub pociągnięcia, na podstawie wcześniej zdefiniowanych zasad lub cech. Wykorzystywane są takie techniki jak projekcja profilu, metody konturowe, szkieletyzacja i dopasowywanie szablonów. Metody profilu projekcji są często używane w językach, takich jak arabski, gdzie litery w słowach są połączone. Metoda ta polega na analizie segmentu słowa, aby znaleźć miejsca, gdzie można podzielić tekst na poszczególne znaki. Wykorzystuje ona fakt, że linie łączące sąsiadujące litery są cieńsze niż same litery, co pomaga w precyzyjnym podziale tekstu. Metody konturowe analizują ogólny kształt słowa, identyfikując kontury reprezentujące zewnętrzny zarys znaków. Dzięki temu można lokalizować punkty segmentacji na podstawie zmian w ich konturze. Dobrze sprawdzają się dla czcionek z ligaturami i nachodzącymi na siebie znakami. Metody szkieletyzacji dzielą tekst na mniejsze jednostki za pomocą operacji takich jak otwieranie (usuwanie małych obiektów  i szumu, wygładzanie krawędzi), zamykanie (wypełnianie małych dziur i łączenie blisko położone obiektów) i odchudzanie (redukcja grubości obiektów). Tworzy się szkielet słów, co redukuje ilość danych do przetworzenia, ale może to prowadzić do utraty informacji i zniekształcenia kształtu znaków. Metody dopasowywania szablonów wykorzystują ruchome okno i wcześniej zdefiniowane szablony znaków, aby znaleźć punkty cięcia znaków. Działają dobrze dla prostych drukowanych tekstów, ale są wrażliwe na różnice w rozmiarze i położeniu znaków oraz są zależne od metod wstępnego przetwarzania \[9\].

Niejawna segmentacja, zwana również segmentacją opartą na rozpoznawaniu, polega na jednoczesnym przeprowadzaniu segmentacji i rozpoznawaniu znaków. Segmenty słów są wyszukiwane w celu znalezienia komponentów, które pasują do wcześniej zdefiniowanych klas w alfabecie. Oznacza to, że segmentacja i rozpoznawanie odbywają się równocześnie, bez wcześniejszego podziału na mniejsze jednostki, takie jak znaki. Metody stosowane w niejawnym procesie segmentacji dzielą się na dwie podkategorie: metodę okienkową i metodę wykrywania cech. Metody okienkowe segmentują obraz słowa, wykorzystując ruchome okno o zmiennej szerokości. Proces ten nie bierze pod uwagę cech obrazu, próbując wybrać optymalną segmentację na podstawie klasyfikacji wygenerowanych pod-obrazów. Metody wykrywania cech skupiają się na fizycznych cechach obrazu, a następnie dzielą je na dobrze skategoryzowane podzbiory w celu porównania z szablonami znaków. Segmentacja niejawna jest prostsza i zwykle niezależna od języka, ale wymaga dużej ilości danych treningowych oraz jest zależna od wydajności klasyfikacji \[9\].

Rozpoznawanie znaków zaczyna się od wydobycia cech z poszczególnych znaków. Kluczowym celem tej operacji jest uzyskanie istotnych informacji, takich jak kształty liter, kontury, linie i zakrzywienia. Na tym etapie obrazy znaków są przetwarzane jako dane wejściowe. W literaturze często stosuje się metody analizy obrazów rastrowych do wyodrębniania istotnych cech.

Najpierw obrazy są poddawane różnym technikom analizy, takim jak filtry Gabor \[16\], Lokalny Wzorzec Binarny (ang. _Local Binary Pattern - LBP_), Histogramy Zorientowanych Gradientów (ang. _Histogram Oriented Gradient - HOG_) oraz cechy zagęszczenia stref (ang. _zoning-based density features_) \[17\]. Filtry Gabor są używane do wykrywania kształtów i tekstur, HOG do analizy gradientów i krawędzi, a cechy zagęszczenia stref do oceny rozkładu pikseli w określonych regionach obrazu. Dzięki tym technikom możliwe jest wyodrębnienie kluczowych cech bezpośrednio z obrazów rastrowych.

Następnie obraz o przykładowych wymiarach 28×28 pikseli przetworzony przez techniki analizy można przedstawić jako macierz wartości binarnych, gdzie "0" oznacza kolor czarny, a "1" kolor biały. Taka macierz 28×28 zostaje przekształcona w wektor wejściowy składający się z 784 elementów \[18\]. W przypadku dużych rozmiarowo macierzy, używana jest metoda analizy głównych składowych (ang. _principal component analysi_s – PCA) w celu redukcji rozmiaru zbioru danych \[16\].

Przefiltrowana macierz jest przekazywana dalej do modułu rozpoznawania znaków, w którym najczęściej stosowaną techniką klasyfikacji znaków są głębokie sieci neuronowe, a dokładniej konwolucyjne sieci neuronowe (ang. _Convolutional Neural Network – CNN_) oraz rekurencyjne sieci neuronowe (ang. _Recurrent Neural Network – RNN_). Przez ostatnie dwie dekady przeprowadzono wiele badań nad technikami klasyfikacji znaków porównując je ze sobą, a sztuczne sieci neuronowe wykazywały się najlepszą skutecznością \[19\].

Przed wprowadzeniem głębokich sieci neuronowych do modułów OCR, najczęściej stosowaną techniką były funkcje jądrowe (ang. _kernel_), wśród których stosowano techniki takie jak maszyny wektorów nośnych (ang. _Support Vector Machine – SVM_), analizę dyskryminacyjną Fishera z funkcjami jądrowymi (ang. _Kernel Fisher Discriminant Analysis_ - _KFDA_) czy analizę głównych składowych z funkcjami jądrowymi (ang. _Kernel Principal Component Analysis – KPCA_) \[19\].

Metody statystyczne także są przedmiotem badań w kontekście klasyfikacji znaków, a jedną z najczęściej stosowanych technik jest algorytm k najbliższych sąsiadów (ang. _k nearest neighbor – kNN_), ceniony dzięki łatwości uczenia modeli statystycznych. W badaniach dotyczących klasyfikacji w OCR swój udział mają także takie techniki metod statystycznych jak: drzewa decyzyjne, regresja logistyczna, liniowa analiza dyskryminacyjna czy ukryty model Harkova \[19\].

Kolejną grupą są metody oparte na dopasowywaniu wzorców, w których używa się zazwyczaj podejścia przesuwającego się okna, w którym predefiniowany obraz lub cecha są przesuwane po obrazie wejściowym w celu określenia podobieństwa między nimi. Stopień podobieństwa między szablonem a obrazem wejściowym jest obliczany przy użyciu różnych formuł matematycznych. Do najczęściej stosowanych miar podobieństwa należą korelacja krzyżowa i korelacja znormalizowana. Natomiast miary odległości obejmują odległość euklidesową i odległość miejską. Proces ten polega na przesuwaniu okna z szablonem po całym obrazie i obliczaniu wartości miar dla każdego przesunięcia. Po porównaniu fragmentu obrazu ze wszystkimi posiadanymi szablonami i otrzymaniu zbioru wyników, znak jest klasyfikowany na podstawie najlepszego wyniku (największa wartość podobieństwa lub najmniejsza wartość odległości) \[19\].

Techniką klasyfikacji używaną przed popularyzacją metod jądrowych i sieci neuronowych było strukturalne rozpoznawanie wzorców. Obrazy w tej technice są dodatkowo poddawane operacjom pozwalającym na wyodrębnienie elementów strukturalnych, takich jak krawędzie, kontury, geometria itp. Stosowaną techniką wyodrębniania elementów ze znaków, tzw. prymitywów, jest na przykład Histogram Kodu Łańcuchowego (ang. _Chain Code Histogram – CCH_). Otrzymane prymitywy są następnie reprezentowane w grafach jako węzły, natomiast krawędzie reprezentują sposób ułożenia prymitywów względem siebie w miejscach styku. Istnieje także inne podejście reprezentacji znaków, tzw. metody oparte na gramatyce, w których to tworzone są ciągi lub drzewa składniowe z zastosowaniem określonych reguł gramatycznych wykorzystując wcześniej wyodrębnione prymitywy. Takie reprezentacje strukturalne są następnie poddawane analizie w celu określenia stopnia podobieństwa pomiędzy wygenerowaną strukturą i strukturą szablonową. W przypadku metod grafowych, do analizy wykorzystywane są zazwyczaj algorytmy podobieństwa grafów, takie jak similarity flooding czy SimRank. Metody oparte na gramatyce wykorzystują koncepcje gramatyki do analizy strukturalnych podobieństw. Proces ten polega na sprawdzeniu zgodności ciągów lub drzew z określonymi regułami gramatycznymi. Taka analiza syntaktyczna pozwala na identyfikację podobieństw między strukturą znaku a wzorcami, prowadząc do skutecznej klasyfikacji znaków \[19\].

Po zakończeniu wstępnego rozpoznawania znaków surowy tekst często zawiera błędy wynikające z nieprawidłowego odczytania znaków, złej jakości obrazu lub zakłóceń w tle. Typowe błędy mogą obejmować zamianę podobnych wizualnie znaków, takich jak „0” i „O” lub „1” i „I”. W celu poprawy dokładności i czytelności ostatecznego wyniku przeprowadza się obróbkę końcową (_ang. Postprocessing_), która polega na zastosowaniu różnych technik i metod korekcji błędów.

Techniki preprocessingowe mają za zadanie przede wszystkim zidentyfikować błędy oraz je naprawić. Inne zadania mogą dotyczyć identyfikacji metadanych, zdań, akapitów czy układu tekstu (dwukolumnowy czy jednokolumnowy) \[20\]. Takimi zadaniami zajmuje się interdyscyplinarna dziedzina przetwarzania języka naturalnego (_ang. NLP – Natural Language Processing_), która łączy zagadnienia nauczania maszynowego i językoznawstwa. Polega ona na wyodrębnianiu istotnych informacji ze zgromadzonych danych językowych oraz opracowywaniu algorytmów i modeli, które potrafią rozumieć i przetwarzać język ludzki w całej jego złożoności. Wraz z rozwojem NLP wykształtowały się dwie główne kategorie metod rozwiązujących problemy przetwarzania języka naturalnego: podejścia oparte na regułach oraz podejścia oparte na uczeniu maszynowym \[20\].

Przed latami 80. XX wieku, przetwarzanie języka naturalnego opierało się głównie na metodach opartych na regułach opracowanych przez lingwistów, w tym technikach struktur syntaktycznych wprowadzonych przez Noama Chomsky'ego. Chomsky zaproponował teorię, że język jest wrodzoną zdolnością ludzkiego umysłu, a jego struktury syntaktyczne miały na celu formalne opisanie gramatyki języków naturalnych. Podejścia oparte na regułach były jednak sztywne i wymagały znacznego nakładu pracy, ponieważ dla każdego języka trzeba było opracować osobne zasady. Ponadto, takie metody miały trudności z uchwyceniem zawiłości języka, co sprawiło, że później, w latach 80., zaczęto wprowadzać algorytmy uczenia maszynowego do przetwarzania języka, co zrewolucjonizowało tę dziedzinę \[20\].

Tradycyjne metody uczenia maszynowego w przetwarzaniu języka naturalnego były stosowane przez wiele lat aż do rozwoju głębokich sieci neuronowych, ale i w obecnych czasach są nadal wartościowe i znajdują swoje zastosowanie w sytuacjach ograniczonych danych treningowych. Metody tradycyjne wykorzystują modele statystyczne i algorytmy do nauki z danych wejściowych oraz ich relacji do wyników. Podobnie jak w przypadku metod opartych na regułach, eksperci musieli spędzać dużo czasu na ręcznym wybieraniu i przygotowywaniu odpowiednich cech z surowych danych, co było trudne i czasochłonne. Jakość cech miała istotny wpływ na działanie modelu. Tradycyjne metody przetwarzania języka naturalnego miały problem z radzeniem sobie ze skomplikowanymi strukturami języka, jak sarkazm czy ironia, ponieważ polegały na ręcznie tworzonych cechach, ale są bardziej wydajne podczas treningu niż sieci neuronowe, co prowadzi do krótszych czasów treningu i mniejszych wymagań pamięciowych. Są również przydatne, gdy mamy mało danych treningowych. Przykładami tradycyjnych technik są naiwne klasyfikatory bayesowskie, algorytmy k najbliższych sąsiadów (k-NN) czy maszyny wektorów nośnych (SVM). W kontekście przetwarzania OCR tradycyjne techniki uczenia maszynowego uczą się z cech takich jak podobieństwo ciągów, popularność języka i istnienie leksykonu, aby poprawić wybór kandydatów i uszeregować ich na podstawie prawdopodobieństwa poprawnej korekcji błędów \[20\].

Metody oparte na sieciach neuronowych różnią się od tradycyjnych metod, ponieważ nie wymagają ręcznej inżynierii cech, co sprawia, że proces treningu jest bardziej efektywny. Aktualnie sieci neuronowe są najczęściej stosowaną techniką przetwarzania języka naturalnego w OCR. Sieci neuronowe potrafią uczyć się z surowych danych dzięki zdolności do obsługi złożonych struktur językowych, przez co są bardziej odpowiednie do zadań wymagających głębszego zrozumienia tekstu, takich jak analiza sentymentu czy tłumaczenie języków. Modele oparte na głębokich sieciach neuronowych w NLP obejmują sieci rekurencyjne (RNN), splotowe sieci neuronowe (CNN), sieci długiej i krótkiej pamięci (LSTM) oraz modele oparte na transformatorach, takie jak BERT i GPT \[20\].

Podsumowując, dziedzina NLP rozwijała się od podejść opartych na regułach po zaawansowane modele oparte na głębokim uczeniu, takie jak BERT czy GPT, a nowoczesne modele pozwalają na znacznie dokładniejsze przetwarzanie tekstu, co ma kluczowe znaczenie dla skuteczności technologii OCR. Przedstawione w niniejszym rozdziale etapy opisują kompozycję wielu technik używanych obecnie i w przeszłości do przeprowadzania procesu OCR. Rozbudowa procesu OCR o różnorodne techniki zawsze miała na celu zwiększenie skuteczności optycznego rozpoznawania znaków, co doprowadziło do stopniowego wzrostu skomplikowania tego procesu. Różnorodność technik wykorzystywanych w rozpoznawaniu znaków sprawia, że rozwiązania OCR mogą znacząco różnić się pod względem konstrukcji i zastosowań, a jednocześnie umożliwiają odnajdywanie nisz, w których są szczególnie skuteczne.

# Bibliografia


| Nr | Źródło|
| --- | --- |
| \[1\] | A. L. Somashekharaiah i A. Deshpande, „Preprocessing techniques for recognition of ancient Kannada epigraphs,” _International Journal of Electrical and Computer Engineering (IJECE),_ pp. 358-365, 2024. |
| \[2\] | J. Wang, „A Study of The OCR Development History and Directions of Development.,” _Highlights in Science, Engineering and Technology. 72.,_ pp. 409-415, 2023. |
| \[3\] | P. Shruthi i D. Verma C, „A Detailed Study and Recent Research on OCR.,” _Vol. 19 No. 2 FEBRUARY 2021 International Journal of Computer Science and Information Security (IJCSIS),_ 2021. |
| \[4\] | W. Stroud, A. Barr i E. E. F. D'Albe, „Optophone”. US Patent 1350 954, 1919. |
| \[5\] | G. Tausheck, „Reading Machine”. US Patent 2026330A. |
| \[6\] | P. W. Handel, „Statistical machine”. US Patent 1915 993, 1933. |
| \[7\] | P. Vineetha, „Optical Character Recognition for Images & Video Frames: A Survey.,” _Vol. 6 Issue 1 2018 IJSRD - International Journal for Scientific Research & Development,_ 2018. |
| \[8\] | A. Fateh, M. Fateh i V. Abolghasemi, „Enhancing optical character recognition: Efficient techniques for document layout analysis and text line detection.,” _Engineering Reports.,_ 2024. |
| \[9\] | A. Qaroush, A. Awad, M. Modallal i M. Ziq, „Segmentation-based, omnifont printed Arabic character recognition without font identification.,” _Journal of King Saud University-Computer and Information Sciences 34.6,_ pp. 3025-3039, 2022. |
| \[10\] | S. S. Rawat i A. Sharma, „Analysis of Image Preprocessing Techniques to Improve OCR of Garhwali Text Obtained Using the Hindi Tesseract Model,” _ICTACT Journal on Image & Video Processing 12.2,_ 2021. |
| \[11\] | R. Dey, R. C. Balabantaray, S. Mohanty, D. Singh, M. Karuppiah i D. Samanta, „Approach for preprocessing in offline optical character recognition (ocr).,” _Interdisciplinary Research in Technology and Management (IRTM). IEEE,_ 2022. |
| \[12\] | N. Sadri, J. Desir, W. Khallouli, M. S. Uddin, S. Kovacic, A. Sousa-Poza, M. Cannan i J. Li, „Image Enhancement for Improved OCR and Deck Segmentation in Shipbuilding Document Images.,” _2022 IEEE 13th Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON),_ 2022. |
| \[13\] | N. Sharma, „Evaluation of Different Kernels for Spatial Filtering Techniques Under Image Processing,” _nternational Journal of Advances in Engineering Architecture Science and Technology ,_ pp. 144-147, 2024. |
| \[14\] | R. Lu, Y. Xu i Z. Wu, „Correction of Slanted Text Pictures,” _Journal of Network Intelligence Vol 6,_ pp. 238-246, 2021. |
| \[15\] | H. Choi i J. Jeong, „Realtime detection of table objects and text areas for OCR preprocessing.,” _WSEAS Transactions on Information Science and Applications 20,_ pp. 197-205, 2023. |
| \[16\] | Heena, H. Sharma i A. M. Gupta, „Comprehensive survey on devanagari OCR.,” _In Proceedings of the International Conference on Innovative Computing & Communication (ICICC).,_ 2022. |
| \[17\] | J. Huang, I. Ul Haq, C. Dai, S. Khan, S. Nazir i M. Imtiaz, „Isolated Handwritten Pashto Character Recognition Using a K‐NN Classification Tool based on Zoning and HOG Feature Extraction Techniques.,” _Complexity,_ 2021. |
| \[18\] | P. Wójcicki, „Wybrane zagadnienia metod optycznego rozpoznawania znaków.,” _Prace doktorantów Politechniki Lubelskiej,_ pp. 107-117, 2020. |
| \[19\] | J. Memon, M. Sami, R. A. Khan i A. M. Uddin, „Handwritten Optical Character Recognition (OCR): A Comprehensive Systematic Literature Review (SLR).,” _IEEE,_ 2020. |
| \[20\] | M. Hajiali, OCR Post-processing Using Large Language Models., 2023. |

# NETOGRAFIA
| Nr | Źródło|
| --- | --- |
| N1 | <https://postalmuseum.si.edu/research-articles/machines-or-bust/mail-processing-machines#\_ftn60> (dostęp: 08.10.2024) |
| N2 | <https://www.kurzweiltech.com/kcp.html> (dostęp: 08.10.2024) |

# Autor
Tekst przygotowany przez: Wojciech Marczewski.
